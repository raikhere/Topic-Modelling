{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aee38aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on \n",
    "Wed Nov 23 23:39:42 2022\n",
    "@author: parth\n",
    "\"\"\"\n",
    "\n",
    "from tkinter import *\n",
    "import tkinter as tk\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def find_features(document):\n",
    "    features = []\n",
    "    for i in document:\n",
    "        doc = spmodel(i.lower())\n",
    "        text = [j.lemma_ for j in doc if j.lemma_ not in stop_words]\n",
    "        features.append(text)\n",
    "        \n",
    "    features = tokenizer.texts_to_sequences(features)\n",
    "    features = [[j for j in i if j<=8000]for i in features]\n",
    "    features = pad_sequences(features, maxlen=136)\n",
    "            \n",
    "    return features\n",
    "        \n",
    "        \n",
    "\n",
    "def find_cat():\n",
    "    x = t1.get(1.0, \"end-1c\")\n",
    "    x = x.split(\"///\")\n",
    "    X = find_features(x)\n",
    "    print(X)\n",
    "    print(X.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    y_pred = model.predict(X, verbose=0)\n",
    "    ans = np.argmax(y_pred, axis = 1)\n",
    "    ans = labelencoder.classes_[ans]\n",
    "    \n",
    "    show = \"\"\n",
    "    \n",
    "    for i in range(len(ans)):\n",
    "        show += str(i+1)+\" \"+ans[i]+\"\\n\"\n",
    "    \n",
    "    var.set(show)\n",
    "    \n",
    "def clear():\n",
    "    t1.delete(1.0, \"end-1c\")\n",
    "    var.set(\"\")\n",
    "   \n",
    " \n",
    "spmodel = spacy.load(\"en_core_web_sm\")\n",
    "model = keras.models.load_model(\"3-conv128-3-dense256-1680808526-weighted.h5\")\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "with open(\"tokenizer.pickle\", \"rb\") as file: \n",
    "    tokenizer= pickle.load(file)\n",
    "    \n",
    "with open(\"labelencoder.pickle\", \"rb\") as file:\n",
    "    labelencoder = pickle.load(file)\n",
    "    \n",
    "window = Tk()\n",
    "window.geometry(\"700x700\")\n",
    "\n",
    "var = StringVar()\n",
    "\n",
    "lab2= Label(window , text=\"If want to find category of multiple news use /// to seperate them\", font=(\"Comic Sans MS\", 15, \"bold\"))\n",
    "lab2.grid(column=1, row=0, padx=30, pady=20)\n",
    "    \n",
    "t1 = Text(window, height=15, width=65, font=(\"Comic Sans MS\", 15))\n",
    "t1.grid(column=1, row=3, padx=30)\n",
    "\n",
    "\n",
    "button = Button(window, text=\"Find Category\", command=lambda: find_cat(), font=(\"Comic Sans MS\", 15, \"bold\"))\n",
    "button.place(x=130, y=500)\n",
    "\n",
    "button2 = Button(window, text=\"Clear\", command= lambda: clear(), font=(\"Comic Sans MS\", 15, \"bold\"))\n",
    "button2.place(x=550, y=500)\n",
    "\n",
    "res = Label(window, height=10, width=65, textvariable=var, borderwidth=2, relief=\"solid\", font=(\"Comic Sans MS\", 15, \"bold\"))\n",
    "res.place(x=40, y=550)\n",
    "\n",
    "\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3a4b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e731c0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1201d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9497207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=tokenizer.index_word.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfd1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=list(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5c9853",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp[1:8000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9729dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72a46ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz=temp[1:8001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7623d1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk2=Tokenizer(oov_token=\"<OOV>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ec72f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk2.fit_on_texts(xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7cb73491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tk2.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2966d6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
